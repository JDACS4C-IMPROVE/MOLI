{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f388ddf7-df75-4d89-96e8-3b80da96ba90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import pandas as pd\n",
    "import math\n",
    "import sklearn.preprocessing as sk\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6ae773e-5827-4934-aab8-4580383c7995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x14bb20077a90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_results_to = '/common/statsgeneral/gayara/MOLI/Docetaxel/all_data/results/complete/'\n",
    "max_iter = 50\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b1e9bae-1d72-48c5-82f6-6c21a192beb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GDSCE = pd.read_csv(\"/common/statsgeneral/gayara/MOLI/Docetaxel/all_data/GDSC_exprs.Docetaxel.eb_with.TCGA_exprs.Docetaxel.tsv\", \n",
    "                    sep = \"\\t\", index_col=0, decimal = \",\")\n",
    "GDSCE = pd.DataFrame.transpose(GDSCE)\n",
    "\n",
    "TCGAE = pd.read_csv(\"/common/statsgeneral/gayara/MOLI/Docetaxel/all_data/TCGA_exprs.Docetaxel.eb_with.GDSC_exprs.Docetaxel.tsv\", \n",
    "                   sep = \"\\t\", index_col=0, decimal = \",\")\n",
    "TCGAE = pd.DataFrame.transpose(TCGAE)\n",
    "\n",
    "TCGAM = pd.read_csv(\"/common/statsgeneral/gayara/MOLI/Docetaxel/all_data/TCGA_mutations.Docetaxel.tsv\", \n",
    "                   sep = \"\\t\", index_col=0, decimal = \".\")\n",
    "TCGAM = pd.DataFrame.transpose(TCGAM)\n",
    "TCGAM = TCGAM.loc[:,~TCGAM.columns.duplicated()]\n",
    "\n",
    "TCGAC = pd.read_csv(\"/common/statsgeneral/gayara/MOLI/Docetaxel/all_data/TCGA_CNA.Docetaxel.tsv\", \n",
    "                   sep = \"\\t\", index_col=0, decimal = \".\")\n",
    "TCGAC = pd.DataFrame.transpose(TCGAC)\n",
    "TCGAC = TCGAC.loc[:,~TCGAC.columns.duplicated()]\n",
    "\n",
    "GDSCM = pd.read_csv(\"/common/statsgeneral/gayara/MOLI/Docetaxel/all_data/GDSC_mutations.Docetaxel.tsv\", \n",
    "                    sep = \"\\t\", index_col=0, decimal = \".\")\n",
    "GDSCM = pd.DataFrame.transpose(GDSCM)\n",
    "GDSCM = GDSCM.loc[:,~GDSCM.columns.duplicated()]\n",
    "\n",
    "GDSCC = pd.read_csv(\"/common/statsgeneral/gayara/MOLI/Docetaxel/all_data/GDSC_CNA.Docetaxel.tsv\", \n",
    "                    sep = \"\\t\", index_col=0, decimal = \".\")\n",
    "GDSCC.drop_duplicates(keep='last')\n",
    "GDSCC = pd.DataFrame.transpose(GDSCC)\n",
    "GDSCC = GDSCC.loc[:,~GDSCC.columns.duplicated()]\n",
    "\n",
    "selector = VarianceThreshold(0.05)\n",
    "selector.fit_transform(GDSCE)\n",
    "GDSCE = GDSCE[GDSCE.columns[selector.get_support(indices=True)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f930ccb-a819-4574-a97c-03e2eceaa6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TCGAC = TCGAC.fillna(0)\n",
    "TCGAC[TCGAC != 0.0] = 1\n",
    "TCGAM = TCGAM.fillna(0)\n",
    "TCGAM[TCGAM != 0.0] = 1\n",
    "GDSCM = GDSCM.fillna(0)\n",
    "GDSCM[GDSCM != 0.0] = 1\n",
    "GDSCC = GDSCC.fillna(0)\n",
    "GDSCC[GDSCC != 0.0] = 1\n",
    "\n",
    "ls = set(GDSCE.columns.values).intersection(set(GDSCM.columns.values))\n",
    "ls = set(ls).intersection(set(GDSCC.columns.values))\n",
    "ls = set(ls).intersection(TCGAE.columns)\n",
    "ls = set(ls).intersection(TCGAM.columns)\n",
    "ls = set(ls).intersection(set(TCGAC.columns.values))\n",
    "ls2 = set(GDSCE.index.values).intersection(set(GDSCM.index.values))\n",
    "ls2 = set(ls2).intersection(set(GDSCC.index.values))\n",
    "ls3 = set(TCGAE.index.values).intersection(set(TCGAM.index.values))\n",
    "ls3 = set(ls3).intersection(set(TCGAC.index.values))\n",
    "#ls = pd.unique(ls)\n",
    "\n",
    "TCGAE = TCGAE.loc[ls3,ls]\n",
    "TCGAM = TCGAM.loc[ls3,ls]\n",
    "TCGAC = TCGAC.loc[ls3,ls]\n",
    "GDSCE = GDSCE.loc[ls2,ls]\n",
    "GDSCM = GDSCM.loc[ls2,ls]\n",
    "GDSCC = GDSCC.loc[ls2,ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4375a5c2-ff98-4641-98c1-dccf092fba90",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDSCR = pd.read_csv(\"/common/statsgeneral/gayara/MOLI/Docetaxel/all_data/GDSC_response.Docetaxel.tsv\", \n",
    "                    sep = \"\\t\", index_col=0, decimal = \",\")\n",
    "TCGAR = pd.read_csv(\"/common/statsgeneral/gayara/MOLI/Docetaxel/all_data/TCGA_response.Docetaxel.tsv\", \n",
    "                       sep = \"\\t\", index_col=0, decimal = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da153c3c-dcbc-4879-bcba-cf3f9fcaee6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-0; Total loss: 0.6698\n",
      "Iter-1; Total loss: 0.7184\n",
      "Iter-2; Total loss: 0.6571\n",
      "Iter-3; Total loss: 0.6514\n",
      "Iter-4; Total loss: 0.7309\n",
      "Iter-5; Total loss: 0.6494\n",
      "Iter-6; Total loss: 0.6299\n",
      "Iter-7; Total loss: 0.6451\n",
      "Iter-8; Total loss: 0.6952\n",
      "Iter-9; Total loss: 0.7265\n",
      "Iter-10; Total loss: 0.6347\n",
      "Iter-11; Total loss: 0.5933\n",
      "Iter-12; Total loss: 0.6801\n",
      "Iter-13; Total loss: 0.6942\n",
      "Iter-14; Total loss: 0.6517\n",
      "Iter-15; Total loss: 0.6953\n",
      "Iter-16; Total loss: 0.6669\n",
      "Iter-17; Total loss: 0.6681\n",
      "Iter-18; Total loss: 0.6678\n",
      "Iter-19; Total loss: 0.6396\n",
      "Iter-20; Total loss: 0.7021\n",
      "Iter-21; Total loss: 0.6506\n",
      "Iter-22; Total loss: 0.6508\n",
      "Iter-23; Total loss: 0.63\n",
      "Iter-24; Total loss: 0.6385\n",
      "Iter-25; Total loss: 0.6702\n",
      "Iter-26; Total loss: 0.6416\n",
      "Iter-27; Total loss: 0.671\n",
      "Iter-28; Total loss: 0.6595\n",
      "Iter-29; Total loss: 0.6818\n",
      "Iter-30; Total loss: 0.6735\n",
      "Iter-31; Total loss: 0.6596\n",
      "Iter-32; Total loss: 0.641\n",
      "Iter-33; Total loss: 0.6299\n",
      "Iter-34; Total loss: 0.6931\n",
      "Iter-35; Total loss: 0.6502\n",
      "Iter-36; Total loss: 0.6452\n",
      "Iter-37; Total loss: 0.6031\n",
      "Iter-38; Total loss: 0.6441\n",
      "Iter-39; Total loss: 0.6468\n",
      "Iter-40; Total loss: 0.5897\n",
      "Iter-41; Total loss: 0.6286\n",
      "Iter-42; Total loss: 0.5221\n",
      "Iter-43; Total loss: 0.5888\n",
      "Iter-44; Total loss: 0.6679\n",
      "Iter-45; Total loss: 0.6156\n",
      "Iter-46; Total loss: 0.6289\n",
      "Iter-47; Total loss: 0.6591\n",
      "Iter-48; Total loss: 0.625\n",
      "Iter-49; Total loss: 0.6381\n",
      "Iter-50; Total loss: 0.6361\n",
      "Iter-51; Total loss: 0.6159\n",
      "Iter-52; Total loss: 0.6269\n",
      "Iter-53; Total loss: 0.5265\n",
      "Iter-54; Total loss: 0.6868\n",
      "Iter-55; Total loss: 0.605\n",
      "Iter-56; Total loss: 0.4779\n",
      "Iter-57; Total loss: 0.6096\n",
      "Iter-58; Total loss: 0.644\n",
      "Iter-59; Total loss: 0.6725\n",
      "Iter-60; Total loss: 0.6687\n",
      "Iter-61; Total loss: 0.6262\n",
      "Iter-62; Total loss: 0.6694\n",
      "Iter-63; Total loss: 0.5237\n",
      "Iter-64; Total loss: 0.5923\n",
      "Iter-65; Total loss: 0.6896\n",
      "Iter-66; Total loss: 0.634\n",
      "Iter-67; Total loss: 0.6652\n",
      "Iter-68; Total loss: 0.5667\n",
      "Iter-69; Total loss: 0.6535\n",
      "Iter-70; Total loss: 0.6623\n",
      "Iter-71; Total loss: 0.5777\n",
      "Iter-72; Total loss: 0.6702\n",
      "Iter-73; Total loss: 0.5782\n",
      "Iter-74; Total loss: 0.6585\n",
      "Iter-75; Total loss: 0.5887\n",
      "Iter-76; Total loss: 0.6464\n",
      "Iter-77; Total loss: 0.6081\n",
      "Iter-78; Total loss: 0.6363\n",
      "Iter-79; Total loss: 0.6322\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-1.13.0-py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:1565\u001b[0m, in \u001b[0;36msqueeze\u001b[0;34m(a, axis)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1565\u001b[0m     squeeze \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'squeeze'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 235\u001b[0m\n\u001b[1;32m    232\u001b[0m         costts\u001b[38;5;241m.\u001b[39mappend(lossT)\n\u001b[1;32m    233\u001b[0m         aucts\u001b[38;5;241m.\u001b[39mappend(AUCt)\n\u001b[0;32m--> 235\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcosttr\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-r\u001b[39m\u001b[38;5;124m'\u001b[39m,np\u001b[38;5;241m.\u001b[39msqueeze(costts), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-b\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    236\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal cost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    237\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miterations (per tens)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36msqueeze\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-1.13.0-py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:1567\u001b[0m, in \u001b[0;36msqueeze\u001b[0;34m(a, axis)\u001b[0m\n\u001b[1;32m   1565\u001b[0m     squeeze \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39msqueeze\n\u001b[1;32m   1566\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m-> 1567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapit\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msqueeze\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m squeeze()\n",
      "File \u001b[0;32m/util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-1.13.0-py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:43\u001b[0m, in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     wrap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m, method)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrap:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, mu\u001b[38;5;241m.\u001b[39mndarray):\n",
      "File \u001b[0;32m/util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-1.13.0-py39/lib/python3.9/site-packages/torch/_tensor.py:955\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 955\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "GDSCR.rename(mapper = str, axis = 'index', inplace = True)\n",
    "GDSCR = GDSCR.loc[ls2,:]\n",
    "#GDSCR.loc[GDSCR.iloc[:,0] == 'R','response'] = 0\n",
    "#GDSCR.loc[GDSCR.iloc[:,0] == 'S','response'] = 1\n",
    "\n",
    "TCGAR = TCGAR.loc[ls3,:]\n",
    "#TCGAR.loc[TCGAR.iloc[:,1] == 'R','response'] = 0\n",
    "#TCGAR.loc[TCGAR.iloc[:,1] == 'S','response'] = 1\n",
    "\n",
    "d = {\"R\":0,\"S\":1}\n",
    "GDSCR[\"response\"] = GDSCR.loc[:,\"response\"].apply(lambda x: d[x])\n",
    "TCGAR[\"response\"] = TCGAR.loc[:,\"response\"].apply(lambda x: d[x])\n",
    "\n",
    "Y = GDSCR['response'].values\n",
    "\n",
    "ls_mb_size = [30, 36, 60]\n",
    "ls_h_dim = [1024, 512, 256, 128, 64, 32, 16]\n",
    "ls_z_dim = [128, 64, 32, 16]\n",
    "ls_marg = [0.5, 1, 1.5, 2, 2.5]\n",
    "ls_lr = [0.5, 0.1, 0.05, 0.01, 0.001, 0.005, 0.0005, 0.0001,0.00005, 0.00001]\n",
    "ls_epoch = [20, 50, 10, 15, 30, 40, 60, 70, 80, 90, 100]\n",
    "ls_rate = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "ls_wd = [0.01, 0.001, 0.1, 0.0001]\n",
    "\n",
    "skf = StratifiedKFold(n_splits=7, random_state=42, shuffle = True)\n",
    "    \n",
    "for iters in range(max_iter):\n",
    "    k = 0\n",
    "    mbs = random.choice(ls_mb_size)\n",
    "    hdm = random.choice(ls_h_dim)\n",
    "    zdm = random.choice(ls_z_dim)\n",
    "    lre = random.choice(ls_lr)\n",
    "    lrm = random.choice(ls_lr)\n",
    "    lrc = random.choice(ls_lr)\n",
    "    lrCL = random.choice(ls_lr)\n",
    "    epch = random.choice(ls_epoch)\n",
    "    wd = random.choice(ls_wd)\n",
    "    rate = random.choice(ls_rate)\n",
    "    \n",
    "    for train_index, test_index in skf.split(GDSCE.values, Y):\n",
    "        k = k + 1\n",
    "        X_trainE = GDSCE.values[train_index,:]\n",
    "        X_testE =  GDSCE.values[test_index,:]\n",
    "        X_trainM = GDSCM.values[train_index,:]\n",
    "        X_testM = GDSCM.values[test_index,:]\n",
    "        X_trainC = GDSCC.values[train_index,:]\n",
    "        X_testC = GDSCM.values[test_index,:]\n",
    "        y_trainE = Y[train_index]\n",
    "        y_testE = Y[test_index]\n",
    "        \n",
    "        scalerGDSC = sk.StandardScaler()\n",
    "        scalerGDSC.fit(X_trainE)\n",
    "        X_trainE = scalerGDSC.transform(X_trainE)\n",
    "        X_testE = scalerGDSC.transform(X_testE)\n",
    "\n",
    "        X_trainM = np.nan_to_num(X_trainM)\n",
    "        X_trainC = np.nan_to_num(X_trainC)\n",
    "        X_testM = np.nan_to_num(X_testM)\n",
    "        X_testC = np.nan_to_num(X_testC)\n",
    "        \n",
    "        TX_testE = torch.FloatTensor(X_testE)\n",
    "        TX_testM = torch.FloatTensor(X_testM)\n",
    "        TX_testC = torch.FloatTensor(X_testC)\n",
    "        ty_testE = torch.FloatTensor(y_testE.astype(int))\n",
    "        \n",
    "        #Train\n",
    "        class_sample_count = np.array([len(np.where(y_trainE==t)[0]) for t in np.unique(y_trainE)])\n",
    "        weight = 1. / class_sample_count\n",
    "        samples_weight = np.array([weight[t] for t in y_trainE])\n",
    "\n",
    "        samples_weight = torch.from_numpy(samples_weight)\n",
    "        sampler = WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight), replacement=True)\n",
    "\n",
    "        mb_size = mbs\n",
    "\n",
    "        trainDataset = torch.utils.data.TensorDataset(torch.FloatTensor(X_trainE), torch.FloatTensor(X_trainM), \n",
    "                                                      torch.FloatTensor(X_trainC), torch.FloatTensor(y_trainE.astype(int)))\n",
    "\n",
    "        trainLoader = torch.utils.data.DataLoader(dataset = trainDataset, batch_size=mb_size, shuffle=False, num_workers=1, sampler = sampler)\n",
    "\n",
    "        n_sampE, IE_dim = X_trainE.shape\n",
    "        n_sampM, IM_dim = X_trainM.shape\n",
    "        n_sampC, IC_dim = X_trainC.shape\n",
    "\n",
    "        h_dim = hdm\n",
    "        Z_dim = zdm\n",
    "        Z_in = h_dim + h_dim + h_dim\n",
    "        lrE = lre\n",
    "        lrM = lrm\n",
    "        lrC = lrc\n",
    "        epoch = epch\n",
    "\n",
    "        costtr = []\n",
    "        auctr = []\n",
    "        costts = []\n",
    "        aucts = []\n",
    "\n",
    "        class AEE(nn.Module):\n",
    "            def __init__(self):\n",
    "                super(AEE, self).__init__()\n",
    "                self.EnE = torch.nn.Sequential(\n",
    "                    nn.Linear(IE_dim, h_dim),\n",
    "                    nn.BatchNorm1d(h_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout())\n",
    "            def forward(self, x):\n",
    "                output = self.EnE(x)\n",
    "                return output\n",
    "\n",
    "        class AEM(nn.Module):\n",
    "            def __init__(self):\n",
    "                super(AEM, self).__init__()\n",
    "                self.EnM = torch.nn.Sequential(\n",
    "                    nn.Linear(IM_dim, h_dim),\n",
    "                    nn.BatchNorm1d(h_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout())\n",
    "            def forward(self, x):\n",
    "                output = self.EnM(x)\n",
    "                return output    \n",
    "\n",
    "\n",
    "        class AEC(nn.Module):\n",
    "            def __init__(self):\n",
    "                super(AEC, self).__init__()\n",
    "                self.EnC = torch.nn.Sequential(\n",
    "                    nn.Linear(IM_dim, h_dim),\n",
    "                    nn.BatchNorm1d(h_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout())\n",
    "            def forward(self, x):\n",
    "                output = self.EnC(x)\n",
    "                return output      \n",
    "        class Classifier(nn.Module):\n",
    "            def __init__(self):\n",
    "                super(Classifier, self).__init__()\n",
    "                self.FC = torch.nn.Sequential(\n",
    "                    nn.Linear(Z_in, Z_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(rate),\n",
    "                    nn.Linear(Z_dim, 1),\n",
    "                    nn.Dropout(rate),\n",
    "                    nn.Sigmoid())\n",
    "            def forward(self, x):\n",
    "                return self.FC(x)\n",
    "        \n",
    "        torch.cuda.manual_seed_all(42)\n",
    "\n",
    "        AutoencoderE = AEE()\n",
    "        AutoencoderM = AEM()\n",
    "        AutoencoderC = AEC()\n",
    "\n",
    "        solverE = optim.Adagrad(AutoencoderE.parameters(), lr=lrE)\n",
    "        solverM = optim.Adagrad(AutoencoderM.parameters(), lr=lrM)\n",
    "        solverC = optim.Adagrad(AutoencoderC.parameters(), lr=lrC)\n",
    "\n",
    "        Clas = Classifier()\n",
    "        SolverClass = optim.SGD(Clas.parameters(), lr=lrCL, weight_decay = wd)\n",
    "        C_loss = torch.nn.BCELoss()\n",
    "\n",
    "        for it in range(epoch):\n",
    "\n",
    "            epoch_cost4 = 0\n",
    "            epoch_cost3 = []\n",
    "            num_minibatches = int(n_sampE / mb_size) \n",
    "\n",
    "            for i, (dataE, dataM, dataC, target) in enumerate(trainLoader):\n",
    "                flag = 0\n",
    "                AutoencoderE.train()\n",
    "                AutoencoderM.train()\n",
    "                AutoencoderC.train()\n",
    "                Clas.train()\n",
    "                \n",
    "                if torch.mean(target)!=0. and torch.mean(target)!=1.:                      \n",
    "\n",
    "                    ZEX = AutoencoderE(dataE)\n",
    "                    ZMX = AutoencoderM(dataM)\n",
    "                    ZCX = AutoencoderC(dataC)\n",
    "\n",
    "                    ZT = torch.cat((ZEX, ZMX, ZCX), 1)\n",
    "                    ZT = F.normalize(ZT, p=2, dim=0)\n",
    "\n",
    "                    Pred = Clas(ZT)\n",
    "                    loss = C_loss(Pred,target.view(-1,1))   \n",
    "\n",
    "                    y_true = target.view(-1,1)\n",
    "                    y_pred = Pred\n",
    "                    AUC = roc_auc_score(y_true.detach().numpy(),y_pred.detach().numpy()) \n",
    "\n",
    "                    solverE.zero_grad()\n",
    "                    solverM.zero_grad()\n",
    "                    solverC.zero_grad()\n",
    "                    SolverClass.zero_grad()\n",
    "\n",
    "                    loss.backward()\n",
    "\n",
    "                    solverE.step()\n",
    "                    solverM.step()\n",
    "                    solverC.step()\n",
    "                    SolverClass.step()\n",
    "                    \n",
    "                    epoch_cost4 = epoch_cost4 + (loss / num_minibatches)\n",
    "                    epoch_cost3.append(AUC)\n",
    "                    flag = 1\n",
    "\n",
    "            if flag == 1:\n",
    "                costtr.append(torch.mean(epoch_cost4))\n",
    "                auctr.append(np.mean(epoch_cost3))\n",
    "                print('Iter-{}; Total loss: {:.4}'.format(it, loss))\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                AutoencoderE.eval()\n",
    "                AutoencoderM.eval()\n",
    "                AutoencoderC.eval()\n",
    "                Clas.eval()\n",
    "\n",
    "                ZET = AutoencoderE(TX_testE)\n",
    "                ZMT = AutoencoderM(TX_testM)\n",
    "                ZCT = AutoencoderC(TX_testC)\n",
    "\n",
    "                ZTT = torch.cat((ZET, ZMT, ZCT), 1)\n",
    "                ZTT = F.normalize(ZTT, p=2, dim=0)\n",
    "\n",
    "                PredT = Clas(ZTT)\n",
    "                lossT = C_loss(PredT,ty_testE.view(-1,1))         \n",
    "\n",
    "                y_truet = ty_testE.view(-1,1)\n",
    "                y_predt = PredT\n",
    "                AUCt = roc_auc_score(y_truet.detach().numpy(),y_predt.detach().numpy())\n",
    "\n",
    "                costts.append(lossT)\n",
    "                aucts.append(AUCt)\n",
    "                \n",
    "                \n",
    "        costtr_vals = []\n",
    "        for iiii in costtr:\n",
    "            costtr_vals.append(iiii.item())\n",
    "                \n",
    "        plt.plot(np.squeeze(costtr), '-r',np.squeeze(costts), '-b')\n",
    "        plt.ylabel('Total cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "\n",
    "        title = 'Cost Docetaxel iter = {}, fold = {}, mb_size = {},  hz_dim[1,2] = ({},{}), lr[E,M,C] = ({}, {}, {}), epoch = {}, wd = {}, lrCL = {}, rate4 = {}'.\\\n",
    "                      format(iters, k, mbs, hdm, zdm , lre, lrm, lrc, epch, wd, lrCL, rate)\n",
    "\n",
    "        plt.suptitle(title)\n",
    "        plt.savefig(save_results_to + title + '.png', dpi = 150)\n",
    "        plt.close()\n",
    "\n",
    "        plt.plot(np.squeeze(auctr), '-r',np.squeeze(aucts), '-b')\n",
    "        plt.ylabel('AUC')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "\n",
    "        title = 'AUC Docetaxel iter = {}, fold = {}, mb_size = {},  hz_dim[1,2] = ({},{}), lr[E,M,C] = ({}, {}, {}), epoch = {}, wd = {}, lrCL = {}, rate4 = {}'.\\\n",
    "                      format(iters, k, mbs, hdm, zdm , lre, lrm, lrc, epch, wd, lrCL, rate)        \n",
    "\n",
    "        plt.suptitle(title)\n",
    "        plt.savefig(save_results_to + title + '.png', dpi = 150)\n",
    "        plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch GPU 1.13 (py39)",
   "language": "python",
   "name": "pytorch-gpu-1.13-py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
